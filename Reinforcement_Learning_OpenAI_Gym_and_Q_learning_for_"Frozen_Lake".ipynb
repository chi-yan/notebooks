{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning - OpenAI Gym and Q-learning for \"Frozen Lake\".ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnjJoqOdFnOtrys3uK3r9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chi-yan/notebooks/blob/master/Reinforcement_Learning_OpenAI_Gym_and_Q_learning_for_%22Frozen_Lake%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the \"Tutorial: An Introduction to Reinforcement Learning Using OpenAI Gym\" by Joy Zhang\n",
        "\n",
        "The tutorial uses the \"Taxi\" game, and the code is modified for \"Frozen Lake\"\n",
        "\n",
        "https://www.gocoder.one/blog/rl-tutorial-with-openai-gym\n",
        "\n",
        "https://colab.research.google.com/drive/1gS2aJo711XJodqqPIVIbzgX1ktZzS8d8?usp=sharing"
      ],
      "metadata": {
        "id": "nyYLhJIhqgn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "def main():\n",
        "\n",
        "    # create Taxi environment\n",
        "    env = gym.make('FrozenLake-v0')\n",
        "\n",
        "    # initialize q-table\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "    # hyperparameters\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.95\n",
        "    epsilon = 1\n",
        "    decay_rate= 0.00005\n",
        "\n",
        "    # training variables\n",
        "    num_episodes = 50000\n",
        "    max_steps = 99# per episode\n",
        "\n",
        "    # training\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # reset the environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        for s in range(max_steps):\n",
        "\n",
        "            # exploration-exploitation tradeoff\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                # explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # exploit\n",
        "                action = np.argmax(qtable[state,:])\n",
        "\n",
        "            # take action and observe reward\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "     #       print(reward)\n",
        "\n",
        "            # Q-learning algorithm\n",
        "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "            # Update to our new state\n",
        "            state = new_state\n",
        "\n",
        "            # if done, finish episode\n",
        "            if done == True:\n",
        "                break\n",
        "\n",
        "        # Decrease epsilon\n",
        "        epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "    print(qtable)\n",
        "    print(f\"Training completed over {num_episodes} episodes\")\n",
        "    input(\"Press Enter to watch trained agent...\")\n",
        "\n",
        "    # watch trained agent\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "\n",
        "    for s in range(max_steps):\n",
        "\n",
        "        print(f\"TRAINED AGENT\")\n",
        "        print(\"Step {}\".format(s+1))\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        print(\"State: \", state)\n",
        "        print(\"Action: \", action)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        print(\"New State: \", new_state)\n",
        "  \n",
        "        rewards += reward\n",
        "\n",
        "        env.render()\n",
        "        print(f\"score: {rewards}\")\n",
        "        state = new_state\n",
        "\n",
        "        if done == True:\n",
        "            if new_state == 15:\n",
        "                print(\"Reached goal\")\n",
        "            else:\n",
        "                print(\"Fell into hole\")\n",
        "            \n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "twzxg5jtPQdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da6181b-09a6-4191-af88-e064ad0b3d3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.20122338e-01 1.21462371e-01 1.18914159e-01 8.26207754e-02]\n",
            " [5.53661973e-03 1.71112713e-02 1.54090577e-02 5.72216330e-02]\n",
            " [4.32662070e-02 4.97717083e-02 4.38507792e-02 5.66620404e-02]\n",
            " [5.20101650e-03 7.19695931e-03 2.30884708e-02 4.53446474e-02]\n",
            " [1.73971841e-01 3.57729856e-02 2.58022726e-02 1.60294975e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.15987876e-03 3.86168596e-05 4.01584264e-02 1.79504738e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.64277692e-01 1.31247795e-02 4.09835225e-03 1.51198238e-01]\n",
            " [5.57797698e-04 2.59010257e-01 5.90020218e-02 1.30634591e-02]\n",
            " [1.59961364e-02 8.00504352e-02 8.99638811e-05 4.20651807e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.13183568e-03 7.97133615e-01 2.84836208e-01 6.51564845e-02]\n",
            " [2.84432800e-01 6.23151537e-01 7.77025364e-01 6.64766639e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Training completed over 50000 episodes\n",
            "Press Enter to watch trained agent...\n",
            "TRAINED AGENT\n",
            "Step 1\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 2\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 3\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 4\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 5\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 6\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 7\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 8\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 9\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 10\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 11\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 12\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 13\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 14\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 15\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 16\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 17\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 18\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 19\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 20\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 21\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 22\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 23\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 24\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 25\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 26\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 27\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 28\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 29\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 30\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 31\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 32\n",
            "State:  3\n",
            "Action:  3\n",
            "New State:  2\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 33\n",
            "State:  2\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 34\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 35\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 36\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 37\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  4\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 38\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 39\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 40\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 41\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 42\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 43\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 44\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 45\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 46\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  4\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 47\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 48\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 49\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 50\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 51\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 52\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 53\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 54\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 55\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 56\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 57\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  0\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 58\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 59\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 60\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 61\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 62\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 63\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  1\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 64\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 65\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  1\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 66\n",
            "State:  1\n",
            "Action:  3\n",
            "New State:  0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 67\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 68\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  8\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 69\n",
            "State:  8\n",
            "Action:  0\n",
            "New State:  4\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 70\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 71\n",
            "State:  0\n",
            "Action:  1\n",
            "New State:  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 72\n",
            "State:  4\n",
            "Action:  0\n",
            "New State:  8\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "score: 0.0\n",
            "TRAINED AGENT\n",
            "Step 73\n",
            "State:  8\n",
            "Action:  0\n",
            "New State:  12\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "score: 0.0\n",
            "Fell into hole\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "def main():\n",
        "\n",
        "    # create Taxi environment\n",
        "    env = gym.make('FrozenLake-v0')\n",
        "\n",
        "    # initialize q-table\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "    # hyperparameters\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.95\n",
        "    epsilon = 1\n",
        "    decay_rate= 0.00005\n",
        "\n",
        "    # training variables\n",
        "    num_episodes = 50000\n",
        "    max_steps = 99# per episode\n",
        "\n",
        "    # training\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # reset the environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        for s in range(max_steps):\n",
        "\n",
        "            # exploration-exploitation tradeoff\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                # explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # exploit\n",
        "                action = np.argmax(qtable[state,:])\n",
        "\n",
        "            # take action and observe reward\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "     #       print(reward)\n",
        "\n",
        "            # Q-learning algorithm\n",
        "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "            # Update to our new state\n",
        "            state = new_state\n",
        "\n",
        "            # if done, finish episode\n",
        "            if done == True:\n",
        "                break\n",
        "\n",
        "        # Decrease epsilon\n",
        "        epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "    print(qtable)\n",
        "    print(f\"Training completed over {num_episodes} episodes\")\n",
        "    input(\"Press Enter to watch trained agent...\")\n",
        "\n",
        "    # watch trained agent\n",
        "    \n",
        "    done = False\n",
        "    rewards = 0\n",
        "    trials = 1000\n",
        "    results = []\n",
        "    turns = []\n",
        "\n",
        "    for i in range(trials):\n",
        "      state = env.reset()\n",
        "\n",
        "      for s in range(max_steps):\n",
        "\n",
        "          action = np.argmax(qtable[state,:])      \n",
        "          new_state, reward, done, info = env.step(action) \n",
        "          rewards += reward\n",
        "          state = new_state\n",
        "\n",
        "          if done == True:\n",
        "              if new_state == 15:\n",
        "                  turns.append(s)\n",
        "                  results.append(1)\n",
        "              else:\n",
        "                  turns.append(s)\n",
        "                  results.append(0)\n",
        "              break\n",
        "    print(turns)\n",
        "    print(\"Games won: \" , sum(results))\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3MAPX-kUZ3",
        "outputId": "432fdfb2-f6cf-406a-a861-d06fc8e5ebc7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.38913142e-01 1.38564315e-01 1.21989625e-01 9.69939959e-02]\n",
            " [3.13435477e-02 4.98928512e-02 4.78817730e-04 3.82748805e-02]\n",
            " [1.79671057e-03 1.06278496e-02 3.63302541e-02 3.75980182e-02]\n",
            " [3.58016920e-03 6.60611953e-07 3.32299448e-02 3.53775038e-02]\n",
            " [5.57064712e-01 6.45881800e-02 4.79903343e-02 4.21381287e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.03029915e-02 8.65254806e-06 2.05249355e-04 1.80063427e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.26139494e-01 5.71522657e-01 1.91137389e-05 5.54814324e-01]\n",
            " [4.54540367e-01 5.29685828e-01 6.50416188e-04 4.33323407e-02]\n",
            " [8.98994203e-01 8.20963986e-02 1.89258867e-08 2.26425337e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.36613243e-01 2.78713993e-01 7.33564195e-01 5.41496509e-06]\n",
            " [4.11887433e-01 9.75772360e-01 1.20591784e-01 9.38693190e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Training completed over 50000 episodes\n",
            "Press Enter to watch trained agent...\n",
            "[17, 13, 49, 22, 16, 10, 15, 4, 32, 8, 8, 17, 23, 19, 26, 5, 36, 3, 6, 10, 10, 11, 16, 3, 14, 3, 11, 16, 41, 18, 12, 13, 7, 9, 23, 31, 9, 3, 11, 24, 5, 22, 2, 11, 11, 16, 37, 16, 9, 12, 46, 7, 20, 16, 42, 2, 4, 10, 27, 2, 3, 21, 19, 5, 25, 10, 21, 16, 39, 14, 24, 2, 13, 7, 4, 30, 20, 7, 10, 21, 4, 8, 18, 3, 8, 19, 23, 20, 12, 3, 9, 15, 6, 6, 15, 17, 7, 14, 23, 5, 9, 10, 8, 6, 5, 10, 9, 18, 13, 9, 9, 23, 15, 7, 16, 35, 27, 16, 6, 4, 2, 8, 5, 13, 45, 16, 17, 2, 8, 12, 6, 16, 7, 10, 16, 13, 9, 9, 10, 3, 14, 22, 19, 9, 4, 18, 11, 7, 6, 6, 4, 9, 20, 23, 15, 12, 12, 28, 11, 19, 4, 10, 19, 5, 28, 21, 6, 8, 13, 30, 18, 38, 13, 7, 16, 4, 6, 6, 18, 9, 12, 36, 15, 11, 18, 39, 9, 14, 23, 43, 9, 11, 19, 20, 9, 17, 16, 4, 8, 18, 13, 22, 29, 5, 25, 22, 5, 5, 9, 7, 14, 3, 10, 14, 8, 10, 13, 5, 9, 3, 10, 9, 6, 14, 8, 2, 21, 12, 8, 13, 4, 12, 21, 13, 9, 2, 24, 15, 55, 6, 7, 2, 16, 13, 39, 7, 27, 7, 4, 13, 3, 8, 15, 3, 11, 11, 6, 26, 2, 30, 17, 22, 15, 19, 23, 17, 8, 11, 6, 4, 16, 7, 16, 15, 10, 11, 36, 22, 12, 27, 13, 16, 24, 13, 4, 17, 15, 7, 7, 27, 42, 15, 12, 16, 41, 4, 6, 8, 11, 13, 24, 3, 8, 5, 10, 21, 20, 14, 12, 16, 5, 7, 24, 28, 21, 2, 20, 9, 14, 22, 12, 10, 12, 28, 7, 6, 16, 8, 10, 25, 10, 12, 14, 19, 32, 15, 5, 17, 2, 8, 10, 2, 9, 4, 20, 13, 42, 20, 4, 10, 6, 6, 3, 13, 8, 32, 57, 6, 13, 9, 2, 8, 27, 7, 27, 14, 4, 11, 7, 16, 3, 9, 30, 12, 19, 21, 26, 13, 3, 8, 2, 15, 17, 18, 23, 20, 6, 12, 3, 49, 19, 6, 11, 11, 4, 27, 6, 6, 5, 11, 7, 26, 35, 7, 21, 14, 6, 11, 9, 10, 8, 25, 16, 6, 9, 18, 9, 14, 24, 37, 9, 19, 7, 5, 10, 10, 18, 3, 11, 16, 4, 19, 44, 6, 8, 12, 19, 3, 22, 5, 10, 17, 5, 33, 15, 7, 10, 2, 8, 39, 8, 7, 11, 20, 14, 14, 2, 9, 19, 13, 34, 36, 18, 13, 41, 22, 6, 12, 10, 5, 11, 18, 4, 11, 5, 20, 5, 3, 13, 7, 20, 47, 15, 7, 6, 28, 7, 13, 16, 9, 3, 23, 10, 5, 20, 5, 4, 16, 11, 35, 13, 12, 17, 3, 45, 15, 6, 9, 7, 16, 3, 6, 8, 36, 4, 14, 11, 27, 6, 6, 27, 3, 9, 11, 8, 18, 6, 17, 3, 9, 3, 26, 13, 6, 8, 7, 5, 16, 18, 6, 5, 5, 16, 7, 37, 37, 9, 8, 12, 9, 3, 17, 15, 11, 3, 6, 6, 17, 16, 14, 16, 9, 39, 6, 2, 5, 2, 3, 10, 13, 6, 10, 4, 7, 14, 34, 40, 19, 20, 23, 3, 3, 14, 12, 8, 13, 10, 7, 7, 8, 7, 6, 10, 10, 2, 5, 26, 12, 4, 15, 6, 9, 6, 4, 19, 29, 10, 8, 12, 10, 20, 53, 3, 6, 15, 11, 10, 65, 3, 25, 32, 11, 4, 7, 25, 9, 10, 20, 4, 37, 2, 6, 9, 3, 4, 6, 46, 11, 10, 27, 7, 16, 10, 2, 17, 16, 3, 13, 72, 12, 27, 9, 11, 19, 19, 23, 3, 29, 11, 27, 2, 2, 11, 7, 14, 7, 11, 3, 26, 7, 14, 11, 40, 2, 3, 13, 5, 13, 13, 5, 8, 13, 20, 6, 6, 4, 19, 13, 13, 19, 14, 2, 10, 7, 21, 46, 22, 15, 4, 5, 22, 13, 21, 4, 11, 5, 5, 5, 5, 3, 6, 10, 14, 4, 24, 3, 4, 13, 3, 23, 19, 20, 8, 9, 4, 2, 3, 21, 7, 8, 12, 4, 5, 2, 6, 14, 33, 34, 18, 23, 17, 16, 3, 2, 31, 4, 9, 10, 21, 35, 11, 14, 13, 15, 3, 10, 5, 6, 21, 12, 15, 16, 26, 7, 22, 15, 34, 13, 46, 35, 4, 9, 10, 16, 10, 3, 4, 5, 3, 38, 10, 6, 5, 9, 2, 14, 15, 4, 20, 7, 17, 15, 38, 19, 10, 12, 27, 7, 8, 9, 34, 16, 5, 17, 14, 41, 7, 16, 4, 16, 10, 7, 18, 6, 10, 80, 9, 28, 15, 26, 4, 16, 15, 6, 7, 5, 4, 4, 13, 16, 12, 13, 12, 7, 5, 8, 9, 4, 41, 10, 7, 4, 30, 5, 11, 8, 8, 37, 5, 30, 3, 10, 5, 24, 4, 14, 16, 3, 17, 7, 19, 3, 4, 47, 2, 25, 10, 6, 7, 18, 9, 23, 31, 4, 20, 21, 25, 13, 10, 21, 22, 18, 11, 22, 6, 40, 11, 9, 12, 28, 21, 24, 9, 11, 25, 28, 35, 12, 21, 4, 14, 4, 22, 13, 5, 8, 13, 12, 21, 26, 3, 11, 8, 9, 4, 15, 4, 27, 12, 5, 2, 27, 6, 19, 29, 3, 61, 7, 11, 24, 3, 8, 7, 2, 17, 24, 3, 16, 40, 7, 6, 9, 8, 12, 26, 42, 13, 29, 18, 31, 6, 9, 2, 16, 14, 7, 37, 3, 18, 8, 18, 20, 9, 23, 15, 10, 6, 8, 3, 29, 7, 3, 10, 9, 26, 6, 16, 13, 8, 9, 36, 54, 11, 12, 5, 15, 19, 11, 12, 8, 9, 10, 29, 16, 13, 32, 21, 19, 2, 8]\n",
            "Games won:  214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6vXLgjzrnFtb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}